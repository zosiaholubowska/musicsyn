Index: musicsyn/pilot_musicsyn_anotated.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\r\nimport subprocess\r\nimport itertools\r\nimport pickle\r\nimport numpy\r\nfrom numpy.random import default_rng\r\nimport matplotlib.pyplot as plt\r\nimport pandas\r\nimport slab\r\nfrom musicsyn.balanced_sequence import balanced_sequence\r\nfrom musicsyn.subject_data import subject_data\r\nimport random\r\nimport os\r\nimport freefield\r\npath = 'C:\\\\projects\\\\musicsyn'\r\n# os.chdir('C:\\\\projects\\\\musicsyn')\r\nplt.ion()  # enable interactive mode - interactive mode will be on, figures will automatically be shown\r\nrandgenerator = default_rng()  # generator of random numbers\r\nils = pickle.load(open(path + \"/musicsyn/ils.pickle\", \"rb\"))  # load interaural level spectrum ???\r\n\r\n\r\ndef read_melody(file):\r\n    \"\"\"\r\n    This function reads a csv file with the description of notes\r\n    - score data (from csv file)\r\n    - note onsets\r\n    - note frequencies\r\n    - note durations\r\n    - if the note is at the phrase boundary\r\n    \"\"\"\r\n    score_data = pandas.read_csv(file, sep=\";\")  # open the csv file with notes\r\n    onsets = score_data.onset_sec.to_list()  # list of onsets of consecutive notes\r\n    frequencies = score_data.freq.to_list()  # frequencies of consecutive notes\r\n    durations = score_data.duration_sec.to_list()  # note durations\r\n    changable_notes = score_data.changable_note.to_list()  # if at note is possible to change direction\r\n    boundaries = (\r\n        score_data.boundary.to_list()\r\n    )  # 0 or 1 indication if the note is the beginning of a new phrase\r\n    return onsets, frequencies, durations, boundaries, changable_notes\r\n\r\n\r\ndef expenv(n_samples):\r\n    \"\"\"\r\n    It is to create more natural sound\r\n    \"\"\"\r\n    t = numpy.linspace(\r\n        start=0, stop=1, num=n_samples\r\n    )  # returns evenly spaced numbers over a specified interval\r\n    return slab.Signal(numpy.exp(-(0.69 * 5) * t))  # what these numbers specify?\r\n\r\n\r\ndef note(f0, duration):\r\n    \"\"\"\r\n    This is to create actual sound\r\n    \"\"\"\r\n    sig = slab.Sound.harmoniccomplex(f0, duration)\r\n    env = expenv(sig.n_samples)\r\n    sig = sig * env\r\n    return slab.Binaural(sig.ramp())\r\n\r\n\r\ndef play(stim):\r\n    \"\"\"\r\n    Plays a created note\r\n    \"\"\"\r\n    stim.write(\"tmp.wav\")\r\n    subprocess.Popen([\"afplay\", \"tmp.wav\"])\r\n\r\ndef run(melody_file, subject):\r\n    file = slab.ResultsFile(\r\n        subject\r\n    )  # here we name the results folder with subject name\r\n    file_name = file.name\r\n\r\n    onsets, frequencies, durations, boundaries, changable_notes = read_melody(path + f\"/stimuli/{melody_file}\")  # reading the csv file with the information about the notes\r\n    seq = balanced_sequence(boundaries, changable_notes, subject, melody_file)\r\n    # depending of number of boundaries, we are creating a sequence\r\n    directions = itertools.cycle(\r\n        [0, 20]\r\n    )\r\n    directions = itertools.cycle(\r\n        [0, 17.5, -17.5]\r\n    )\r\n    # iterator, which will return the numbers from 0 to 20 in a infinite loop\r\n    # but this iterator does only 0 and 20, shouldn't we have -20, as well?\r\n    # direction_jitter = 5\r\n    start_time = time.time()  # creates a timestamp in Unix format\r\n    # setup the figure for button capture\r\n    fig = plt.figure(\"stairs\")  # I cannot see the figure - for check later\r\n\r\n    def on_key(event):\r\n        print(\"write key: \", event.key)\r\n        file.write(\r\n            event.key, tag=f\"{time.time() - start_time:.3f}\"\r\n        )  # logs the key that was pressed on a specified time\r\n\r\n    cid = fig.canvas.mpl_connect(\"key_press_event\", on_key)\r\n    onsets.append(\r\n        onsets[-1] + durations[-1] + 0.1\r\n    )  # add a dummy onset so that the if statement below works during the last note\r\n    # durations.append(0.1)  ###\r\n    i = 0\r\n    direction = next(directions)\r\n    try:\r\n        while time.time() - start_time < onsets[-1] + durations[-1]:\r\n            if time.time() - start_time > onsets[i]:  # play the next note\r\n                #print(i)\r\n                stim = note(frequencies[i], durations[i])\r\n                if seq[\"sequence\"][i] == 1:\r\n                    direction = next(directions)  # toggle direction\r\n                    print(\"direction change\")\r\n                if seq[\"boundary\"][i]:  # so if there is 1 in the boundaries list\r\n                    print(\"at boundary!\")\r\n                if seq[\"cue\"][i] == 1:\r\n                    print(\"########\")\r\n                    print(\"########\")\r\n                    print(\"visual cue!\")\r\n                    print(\"########\")\r\n                    print(\"########\")\r\n                # direction_addon = randgenerator.uniform(low=-direction_jitter / 2, high=direction_jitter / 2)\r\n                # it creates jitter for the change of the location ranging from -2,5 to 2,5\r\n                #print(direction + direction_addon)\r\n                # stim = stim.at_azimuth(\r\n                #     direction + direction_addon, ils\r\n                # )  # this matches the azimuth with the ils values\r\n                # stim = (\r\n                #     stim.externalize()\r\n                # )  # smooths the sound with HRTF, to simulate external sound source\r\n                # file.write(frequencies[i], tag=f\"{time.time() - start_time:.3f}\")\r\n                speaker = (direction, 0)\r\n                # dont do this online\r\n                stim.level = 70\r\n                stim = stim.resample(samplerate=48828)\r\n\r\n                freefield.set_signal_and_speaker(signal=stim.channel(0), speaker=speaker, equalize=False)\r\n                freefield.play()\r\n                # play(stim)\r\n                i += 1\r\n            plt.pause(0.01)\r\n    except IndexError:\r\n        subject_data(subject, file, melody_file)\r\n\r\n\r\ndef select_file():\r\n    files = [\"stim_maj_1.csv\", \"stim_maj_2.csv\", \"stim_maj_3.csv\"]\r\n    random.shuffle(files)\r\n\r\n    for file in files:\r\n        print(file)\r\n        run(file, 'FH')\r\n\r\n        user_input = input(\"Do you want to continue? (y/n): \")\r\n        if user_input.lower() == 'n':\r\n            break\r\n        elif user_input.lower() == 'y':\r\n            print(\"Continuing...\")\r\n\r\nif __name__ == \"__main__\":\r\n    freefield.initialize('dome', default='play_rec')\r\n\r\n    select_file()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/musicsyn/pilot_musicsyn_anotated.py b/musicsyn/pilot_musicsyn_anotated.py
--- a/musicsyn/pilot_musicsyn_anotated.py	(revision 295090059ec427b8dc7a33e87c5a0f3038e9e2ce)
+++ b/musicsyn/pilot_musicsyn_anotated.py	(date 1698942699265)
@@ -58,6 +58,23 @@
     sig = sig * env
     return slab.Binaural(sig.ramp())
 
+def create_melody(file):
+    samplerate = 8000
+    slab.set_default_samplerate(samplerate)
+    score_data = pandas.read_csv(file, sep=";")  # open the csv file with notes
+    onsets = score_data.onset_sec.to_list()  # list of onsets of consecutive notes
+    frequencies = score_data.freq.to_list()  # frequencies of consecutive notes
+    durations = score_data.duration_sec.to_list()  # note durations
+    total_duration = onsets[-1] + durations[-1]
+    total_samples = int(round(total_duration, 3) * samplerate)
+    data = numpy.zeros(total_samples)
+    # concatenate notes to a melody
+    for i in range(len(frequencies)):
+        onset = int(onsets[i] * samplerate)
+        duration = int(durations[i] * samplerate)
+        data[onset:onset+duration] = note(frequencies[i], durations[i]).data[:, 0]
+    melody = slab.Binaural(data)
+    return melody
 
 def play(stim):
     """
@@ -72,19 +89,20 @@
     )  # here we name the results folder with subject name
     file_name = file.name
 
-    onsets, frequencies, durations, boundaries, changable_notes = read_melody(path + f"/stimuli/{melody_file}")  # reading the csv file with the information about the notes
+    onsets, frequencies, durations, boundaries, changable_notes = read_melody(path + f"/stimuli/{melody_file}")
+    # reading the csv file with the information about the notes
     seq = balanced_sequence(boundaries, changable_notes, subject, melody_file)
     # depending of number of boundaries, we are creating a sequence
-    directions = itertools.cycle(
-        [0, 20]
-    )
+    # directions = itertools.cycle(
+    #     [0, 20]
+    # )
     directions = itertools.cycle(
         [0, 17.5, -17.5]
     )
     # iterator, which will return the numbers from 0 to 20 in a infinite loop
     # but this iterator does only 0 and 20, shouldn't we have -20, as well?
     # direction_jitter = 5
-    start_time = time.time()  # creates a timestamp in Unix format
+
     # setup the figure for button capture
     fig = plt.figure("stairs")  # I cannot see the figure - for check later
 
@@ -101,11 +119,18 @@
     # durations.append(0.1)  ###
     i = 0
     direction = next(directions)
+    # new idea: load the whole melody into the buffer and only change speakers during playback
+    speaker = (direction, 0)
+    melody = create_melody(path + f"/stimuli/{melody_file}")
+    melody = melody.resample(4884)
+    freefield.set_signal_and_speaker(signal=melody, speaker=speaker, equalize=False)
+    start_time = time.time()  # creates a timestamp in Unix format
     try:
+        freefield.play()
         while time.time() - start_time < onsets[-1] + durations[-1]:
             if time.time() - start_time > onsets[i]:  # play the next note
                 #print(i)
-                stim = note(frequencies[i], durations[i])
+                # stim = note(frequencies[i], durations[i])
                 if seq["sequence"][i] == 1:
                     direction = next(directions)  # toggle direction
                     print("direction change")
@@ -129,10 +154,10 @@
                 # file.write(frequencies[i], tag=f"{time.time() - start_time:.3f}")
                 speaker = (direction, 0)
                 # dont do this online
-                stim.level = 70
-                stim = stim.resample(samplerate=48828)
+                # stim.level = 70
+                # stim = stim.resample(samplerate=48828)
 
-                freefield.set_signal_and_speaker(signal=stim.channel(0), speaker=speaker, equalize=False)
+                freefield.set_speaker(speaker=speaker)
                 freefield.play()
                 # play(stim)
                 i += 1
