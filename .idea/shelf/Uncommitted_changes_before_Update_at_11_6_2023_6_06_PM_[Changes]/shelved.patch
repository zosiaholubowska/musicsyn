Index: musicsyn/subject_data.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import slab\r\nimport pandas\r\nimport numpy\r\nimport os\r\n\r\n\r\ndef subject_data(subject, file, melody_file):\r\n    \"\"\"\r\n    This is to create a dataframe with results and the sequence\r\n    \"\"\"\r\n\r\n    file_name = file.name\r\n    data = slab.ResultsFile.read_file(\r\n          os.getcwd() + f\"/Results/{subject}/{file_name}\"\r\n        )\r\n\r\n    timestamps = [float(list(d.keys())[0]) for d in data]\r\n    frequencies = [list(d.values())[0] for d in data]\r\n    responses = numpy.zeros_like(frequencies)\r\n\r\n\r\n\r\n    df = pandas.DataFrame(\r\n        {\"Timestamp\": timestamps, \"Frequency\": frequencies, \"Responses\": responses}\r\n    )\r\n\r\n\r\n    answers = df[df[\"Frequency\"].astype(str).str.isalpha()]\r\n    ans = answers[\"Timestamp\"].tolist()\r\n\r\n    freq = df[~df[\"Frequency\"].astype(str).str.isalpha()]\r\n\r\n    freq = freq.reset_index(drop=True)\r\n\r\n    for row in freq.index[:-1]:\r\n        start = freq.loc[row][0]\r\n        end = freq.loc[row + 1][0]\r\n        if row < len(freq):\r\n            if any(start <= x < end for x in ans if isinstance(x, (int, float))):\r\n                freq.at[row, \"Responses\"] = 1\r\n            else:\r\n                freq.at[row, \"Responses\"] = 0\r\n\r\n    start_last = freq.iloc[-1][\"Timestamp\"]\r\n    if any(\r\n            start_last <= x < (start_last + 1.5) for x in ans if isinstance(x, (int, float))\r\n    ):\r\n        freq.at[len(freq) - 1, \"Responses\"] = 1\r\n    else:\r\n        freq.at[len(freq) - 1, \"Responses\"] = 0\r\n\r\n\r\n\r\n    seq = pandas.read_csv(\r\n      os.getcwd() + f\"/Results/{subject}/{subject}_seq_{melody_file}\"\r\n    )\r\n\r\n    data = seq.join(freq[\"Responses\"])\r\n    data = data.join(freq[\"Frequency\"])\r\n    data = data.join(freq[\"Timestamp\"])\r\n\r\n\r\n\r\n    data = data.drop(columns=[\"idx\"])\r\n    data = data.loc[:, ~data.columns.str.contains(\"^Unnamed\")]\r\n    data = data[[\"Frequency\", \"Timestamp\", \"boundary\", \"sequence\", \"cue\", \"Responses\", ]]\r\n    data = data.rename(\r\n        columns={\r\n            \"boundary\": \"Boundary\",\r\n            \"sequence\": \"Location_change\",\r\n            \"cue\": \"Visual_cue\",\r\n        }\r\n    )\r\n\r\n    data.to_csv(\r\n       os.getcwd() +  f\"/Results/{subject}/{subject}_data_{melody_file}\",\r\n    )\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/musicsyn/subject_data.py b/musicsyn/subject_data.py
--- a/musicsyn/subject_data.py	(revision 295090059ec427b8dc7a33e87c5a0f3038e9e2ce)
+++ b/musicsyn/subject_data.py	(date 1699287791965)
@@ -3,7 +3,7 @@
 import numpy
 import os
 
-
+path = 'C:\\projects\\musicsyn'
 def subject_data(subject, file, melody_file):
     """
     This is to create a dataframe with results and the sequence
@@ -11,7 +11,7 @@
 
     file_name = file.name
     data = slab.ResultsFile.read_file(
-          os.getcwd() + f"/Results/{subject}/{file_name}"
+          path + f"/musicsyn/Results/{subject}/{file_name}"
         )
 
     timestamps = [float(list(d.keys())[0]) for d in data]
@@ -49,10 +49,11 @@
     else:
         freq.at[len(freq) - 1, "Responses"] = 0
 
-
+    subject = 'FH'
+    melody_file = 'stim_maj_1'
 
     seq = pandas.read_csv(
-      os.getcwd() + f"/Results/{subject}/{subject}_seq_{melody_file}"
+      path + f"/musicsyn/Results/{subject}/{subject}_seq_{melody_file}.csv"
     )
 
     data = seq.join(freq["Responses"])
@@ -73,5 +74,5 @@
     )
 
     data.to_csv(
-       os.getcwd() +  f"/Results/{subject}/{subject}_data_{melody_file}",
+       path +  f"/musicsyn/Results/{subject}/{subject}_data_{melody_file}",
     )
Index: musicsyn/pilot_musicsyn_anotated.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\r\nimport subprocess\r\nimport itertools\r\nimport pickle\r\nimport numpy\r\nfrom numpy.random import default_rng\r\nimport matplotlib.pyplot as plt\r\nimport pandas\r\nimport slab\r\nfrom musicsyn.balanced_sequence import balanced_sequence\r\nfrom musicsyn.subject_data import subject_data\r\nimport random\r\nimport os\r\nimport freefield\r\npath = 'C:\\\\projects\\\\musicsyn'\r\n# os.chdir('C:\\\\projects\\\\musicsyn')\r\nplt.ion()  # enable interactive mode - interactive mode will be on, figures will automatically be shown\r\nrandgenerator = default_rng()  # generator of random numbers\r\nils = pickle.load(open(path + \"/musicsyn/ils.pickle\", \"rb\"))  # load interaural level spectrum ???\r\n\r\n\r\ndef read_melody(file):\r\n    \"\"\"\r\n    This function reads a csv file with the description of notes\r\n    - score data (from csv file)\r\n    - note onsets\r\n    - note frequencies\r\n    - note durations\r\n    - if the note is at the phrase boundary\r\n    \"\"\"\r\n    score_data = pandas.read_csv(file, sep=\";\")  # open the csv file with notes\r\n    onsets = score_data.onset_sec.to_list()  # list of onsets of consecutive notes\r\n    frequencies = score_data.freq.to_list()  # frequencies of consecutive notes\r\n    durations = score_data.duration_sec.to_list()  # note durations\r\n    changable_notes = score_data.changable_note.to_list()  # if at note is possible to change direction\r\n    boundaries = (\r\n        score_data.boundary.to_list()\r\n    )  # 0 or 1 indication if the note is the beginning of a new phrase\r\n    return onsets, frequencies, durations, boundaries, changable_notes\r\n\r\n\r\ndef expenv(n_samples):\r\n    \"\"\"\r\n    It is to create more natural sound\r\n    \"\"\"\r\n    t = numpy.linspace(\r\n        start=0, stop=1, num=n_samples\r\n    )  # returns evenly spaced numbers over a specified interval\r\n    return slab.Signal(numpy.exp(-(0.69 * 5) * t))  # what these numbers specify?\r\n\r\n\r\ndef note(f0, duration):\r\n    \"\"\"\r\n    This is to create actual sound\r\n    \"\"\"\r\n    sig = slab.Sound.harmoniccomplex(f0, duration)\r\n    env = expenv(sig.n_samples)\r\n    sig = sig * env\r\n    return slab.Binaural(sig.ramp())\r\n\r\n\r\ndef play(stim):\r\n    \"\"\"\r\n    Plays a created note\r\n    \"\"\"\r\n    stim.write(\"tmp.wav\")\r\n    subprocess.Popen([\"afplay\", \"tmp.wav\"])\r\n\r\ndef run(melody_file, subject):\r\n    file = slab.ResultsFile(\r\n        subject\r\n    )  # here we name the results folder with subject name\r\n    file_name = file.name\r\n\r\n    onsets, frequencies, durations, boundaries, changable_notes = read_melody(path + f\"/stimuli/{melody_file}\")  # reading the csv file with the information about the notes\r\n    seq = balanced_sequence(boundaries, changable_notes, subject, melody_file)\r\n    # depending of number of boundaries, we are creating a sequence\r\n    directions = itertools.cycle(\r\n        [0, 20]\r\n    )\r\n    directions = itertools.cycle(\r\n        [0, 17.5, -17.5]\r\n    )\r\n    # iterator, which will return the numbers from 0 to 20 in a infinite loop\r\n    # but this iterator does only 0 and 20, shouldn't we have -20, as well?\r\n    # direction_jitter = 5\r\n    start_time = time.time()  # creates a timestamp in Unix format\r\n    # setup the figure for button capture\r\n    fig = plt.figure(\"stairs\")  # I cannot see the figure - for check later\r\n\r\n    def on_key(event):\r\n        print(\"write key: \", event.key)\r\n        file.write(\r\n            event.key, tag=f\"{time.time() - start_time:.3f}\"\r\n        )  # logs the key that was pressed on a specified time\r\n\r\n    cid = fig.canvas.mpl_connect(\"key_press_event\", on_key)\r\n    onsets.append(\r\n        onsets[-1] + durations[-1] + 0.1\r\n    )  # add a dummy onset so that the if statement below works during the last note\r\n    # durations.append(0.1)  ###\r\n    i = 0\r\n    direction = next(directions)\r\n    try:\r\n        while time.time() - start_time < onsets[-1] + durations[-1]:\r\n            if time.time() - start_time > onsets[i]:  # play the next note\r\n                #print(i)\r\n                stim = note(frequencies[i], durations[i])\r\n                if seq[\"sequence\"][i] == 1:\r\n                    direction = next(directions)  # toggle direction\r\n                    print(\"direction change\")\r\n                if seq[\"boundary\"][i]:  # so if there is 1 in the boundaries list\r\n                    print(\"at boundary!\")\r\n                if seq[\"cue\"][i] == 1:\r\n                    print(\"########\")\r\n                    print(\"########\")\r\n                    print(\"visual cue!\")\r\n                    print(\"########\")\r\n                    print(\"########\")\r\n                # direction_addon = randgenerator.uniform(low=-direction_jitter / 2, high=direction_jitter / 2)\r\n                # it creates jitter for the change of the location ranging from -2,5 to 2,5\r\n                #print(direction + direction_addon)\r\n                # stim = stim.at_azimuth(\r\n                #     direction + direction_addon, ils\r\n                # )  # this matches the azimuth with the ils values\r\n                # stim = (\r\n                #     stim.externalize()\r\n                # )  # smooths the sound with HRTF, to simulate external sound source\r\n                # file.write(frequencies[i], tag=f\"{time.time() - start_time:.3f}\")\r\n                speaker = (direction, 0)\r\n                # dont do this online\r\n                stim.level = 70\r\n                stim = stim.resample(samplerate=48828)\r\n\r\n                freefield.set_signal_and_speaker(signal=stim.channel(0), speaker=speaker, equalize=False)\r\n                freefield.play()\r\n                # play(stim)\r\n                i += 1\r\n            plt.pause(0.01)\r\n    except IndexError:\r\n        subject_data(subject, file, melody_file)\r\n\r\n\r\ndef select_file():\r\n    files = [\"stim_maj_1.csv\", \"stim_maj_2.csv\", \"stim_maj_3.csv\"]\r\n    random.shuffle(files)\r\n\r\n    for file in files:\r\n        print(file)\r\n        run(file, 'FH')\r\n\r\n        user_input = input(\"Do you want to continue? (y/n): \")\r\n        if user_input.lower() == 'n':\r\n            break\r\n        elif user_input.lower() == 'y':\r\n            print(\"Continuing...\")\r\n\r\nif __name__ == \"__main__\":\r\n    freefield.initialize('dome', default='play_rec')\r\n\r\n    select_file()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/musicsyn/pilot_musicsyn_anotated.py b/musicsyn/pilot_musicsyn_anotated.py
--- a/musicsyn/pilot_musicsyn_anotated.py	(revision 295090059ec427b8dc7a33e87c5a0f3038e9e2ce)
+++ b/musicsyn/pilot_musicsyn_anotated.py	(date 1699289735627)
@@ -12,11 +12,23 @@
 import random
 import os
 import freefield
+import win32com.client
+from pathlib import Path
+
 path = 'C:\\projects\\musicsyn'
 # os.chdir('C:\\projects\\musicsyn')
 plt.ion()  # enable interactive mode - interactive mode will be on, figures will automatically be shown
 randgenerator = default_rng()  # generator of random numbers
 ils = pickle.load(open(path + "/musicsyn/ils.pickle", "rb"))  # load interaural level spectrum ???
+samplerate = 44828
+
+
+rp = win32com.client.Dispatch('RPco.X')
+zb = win32com.client.Dispatch('ZBUS.x')
+rp.ConnectRX8('GB', 1)
+
+rp.ClearCOF()
+rp.LoadCOF(path + 'data/rcx/proto_.rcx')
 
 
 def read_melody(file):
@@ -76,10 +88,7 @@
     seq = balanced_sequence(boundaries, changable_notes, subject, melody_file)
     # depending of number of boundaries, we are creating a sequence
     directions = itertools.cycle(
-        [0, 20]
-    )
-    directions = itertools.cycle(
-        [0, 17.5, -17.5]
+        [18, 1, 9]
     )
     # iterator, which will return the numbers from 0 to 20 in a infinite loop
     # but this iterator does only 0 and 20, shouldn't we have -20, as well?
@@ -105,7 +114,7 @@
         while time.time() - start_time < onsets[-1] + durations[-1]:
             if time.time() - start_time > onsets[i]:  # play the next note
                 #print(i)
-                stim = note(frequencies[i], durations[i])
+                #stim = note(frequencies[i], durations[i])
                 if seq["sequence"][i] == 1:
                     direction = next(directions)  # toggle direction
                     print("direction change")
@@ -126,14 +135,18 @@
                 # stim = (
                 #     stim.externalize()
                 # )  # smooths the sound with HRTF, to simulate external sound source
-                # file.write(frequencies[i], tag=f"{time.time() - start_time:.3f}")
-                speaker = (direction, 0)
-                # dont do this online
-                stim.level = 70
-                stim = stim.resample(samplerate=48828)
+                file.write(frequencies[i], tag=f"{time.time() - start_time:.3f}")
+
+                rp.Run()
 
-                freefield.set_signal_and_speaker(signal=stim.channel(0), speaker=speaker, equalize=False)
-                freefield.play()
+                rp.SetTagVal('f0', frequencies[i])  # write value to tag
+                duration = durations[i] # duration in seconds
+                rp.SetTagVal('len', int(duration * samplerate))
+                rp.SetTagVal('chan', direction)
+
+                rp.Halt()
+
+                zb.zBusTrigA(0, 0, 20)  # trigger zbus
                 # play(stim)
                 i += 1
             plt.pause(0.01)
@@ -156,6 +169,6 @@
             print("Continuing...")
 
 if __name__ == "__main__":
-    freefield.initialize('dome', default='play_rec')
+
 
     select_file()
